<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta changes:
1.  **Updating your `scraper.js`** to include a tiny web server.
2.  **Deploying to Render as a "Web Service"** instead of a "Background Worker".

Let's do it step-by-step.

---

### **Phase 1: Update Your Project Files**

We only need to modify your `scraper.js` file. You don't need a separate `index.html` file; we can serve a simple message directly from the script.

#### **The Final `scraper.js` Code**

This is the complete and final version. It includes the web server logic alongside your existing scraper logic. Copy this entire block and replace the contents of your `scraper.js` file.

```javascript
// =============================================================================
//                            IMPORTS
// =============================================================================
const puppeteer = require('puppeteer');
const axios = require('axios');
const http = require('http'); // Import the built-in HTTP module to create a server

// =============================================================================
//                            CONFIGURATION
// =============================================================================
const CONFIG = {
  TELEGRAM_URL: 'https://t.me/s/Quotex_SuperBot',
  WEBHOOK_URL: process.env.N8N_WEBHOOK_URL, // Reads from environment variable
  SCRAPE_INTERVAL_MS: 5000,
  RELOAD_LIMIT: 10,
  // Render provides the PORT environment variable for Web Services automatically
  PORT: process.env.PORT || 3000,
};

// =============================================================================
//                            HELPER FUNCTIONS
// =============================================================================

async function sendToWebhook(messageText) {
  const payload = { message: messageText };
  try {
    await axios.post(CONFIG.WEBHOOK_URL, payload);
    console.log('✅ Successfully sent message to n8n webhook.');
  } catch (error) {
    console.error(`❌ Failed to send to n8n. Status: ${error.response?.status}`);
    console.error(`   Error details: ${error.message}`);
  }
}

async function createNewPage(browser) {
  console.log('🛠️  Creating a new page...');
  try {
    const page = await browser.newPage();
    await page.setUserAgent('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36');
    await page.goto(CONFIG.TELEGRAM_URL, { waitUntil: 'networkidle2', timeout: 60000 });
    console.log('✅ New page created and navigated successfully.');
    return page;
  } catch (e) {
    console.error('🔥 CRITICAL: Failed to create and initialize a new page.', e.message);
    return null;
  }
}

// =============================================================================
//                         MAIN SCRAPER LOGIC
// =============================================================================

// We wrap the scraper logic in a function so we can call it after the server starts.
async function runScraper() {
  console.log('🚀 Starting the Resilient Puppeteer scraper...');
  const processedMessages = new Set();
  console.log('🖥️  Launching browser...');
  const browser = await puppeteer.launch({
    headless: true,
    args: ['--no-sandbox', '--disable-setuid-sandbox'], // Required for Render
  });

  let page = await createNewPage(browser);
  let reloadCount = 0;

  if (!page) {
    console.log('Could not create initial page. Shutting down scraper logic.');
    await browser.close();
    return;
  }
  
  console.log(` scraper running. Refreshing every ${CONFIG.SCRAPE_INTERVAL_MS / 1000} seconds.`);

  setInterval(async () => {
    try {
      console.log(`🔄 [Count: ${reloadCount + 1}] Reloading page...`);
      await page.reload({ waitUntil: 'networkidle2' });
      reloadCount++;

      const messagesOnPage = await page.$$eval(
        '.tgme_widget_message_text',
        (elements) => elements.map(el => el.innerText.trim())
      );

      for (const text of messagesOnPage) {
        if (text && !processedMessages.has(text)) {
          console.log('-'.repeat(50));
          console.log('📩 NEW MESSAGE DETECTED:');
          console.log(text);
          processedMessages.add(text);
          await sendToWebhook(text);
          console.log('-'.repeat(50));
        }
      }

      if (reloadCount >= CONFIG.RELOAD_LIMIT) {
        console.warn(`♻️ Reached ${CONFIG.RELOAD_LIMIT} reloads. Proactively creating a fresh page.`);
        await page.close();
        page = await createNewPage(browser);
        reloadCount = 0;
        if (!page) throw new Error("Failed to recover by creating a new page after hitting reload limit.");
      }

    } catch (error) {
      console.error('🔥 An error occurred during the scrape cycle:', error.message);
      if (error.message.includes('detached Frame')) {
        console.warn('🚑 Detached frame error detected. Attempting recovery...');
        try {
            page = await createNewPage(browser);
            reloadCount = 0;
            if (!page) throw new Error("Failed to recover from detached frame.");
        } catch (recoveryError) {
            console.error('🔥🔥 CRITICAL FAILURE: Could not recover from detached frame error.', recoveryError.message);
        }
      }
    }
  }, CONFIG.SCRAPE_INTERVAL_MS);
}

// =============================================================================
//                         WEB SERVER FOR KEEP-ALIVE
// =============================================================================

// Create a simple HTTP server that responds to pings from cron-job.org
const server = http.createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('Scraper is alive and running!\n');
});

// Start the server and listen on the port provided by Render
server.listen(CONFIG.PORT, () => {
  console.log(`✅ Health check server listening on port ${CONFIG.PORT}`);
  
  // *** CRUCIAL STEP ***
  // Once the server is successfully listening, we start the main scraper logic.
  runScraper();
});